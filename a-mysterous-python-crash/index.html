<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0,maximum-scale=1,viewport-fit=cover" name=viewport><title>Debugging a mysterious Python crash</title><link href=/print.css media=print rel=stylesheet><link href=/poole.css rel=stylesheet><link href=/hyde.css rel=stylesheet><link href=https://blog.dend.ro/atom.xml rel=alternate title=RSS type=application/atom+xml><link href=https://mapstodon.space/@lnicola rel=me><body><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://blog.dend.ro/><h1></h1></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=/>Home</a><li class=sidebar-nav-item><a href=/content/blogroll/>Blogroll</a><li class=sidebar-nav-item><a href=/content/acknowledgements/>Acknowledgements</a><li class=sidebar-nav-item><a href=https://www.buymeacoffee.com/lnicolaq>Buy me a ☕</a></ul></div></div><div class="content container"><div class=post><h1 class=post-title>Debugging a mysterious Python crash</h1><span class=post-date>2022-05-28</span><p>I recently wanted to prepare a Jupyter notebook with some example code and ran into an interesting problem: trying to display a Matplotlib chart made the IPython kernel crash.<h2 id=gathering-more-info>Gathering more info</h2><p>Luckily, the crash was easy to reproduce outside of Jupyter:<pre class=language-bash data-lang=bash style=background-color:#2b303b;color:#c0c5ce;><code class=language-bash data-lang=bash><span style=color:#bf616a;>$</span><span> python3</span><span style=color:#bf616a;> -c </span><span>'</span><span style=color:#a3be8c;>import matplotlib.pyplot as plt; plt.axes()</span><span>'
</span><span style=color:#bf616a;>Illegal</span><span> instruction
</span></code></pre><p>This usually means one of two things:<ul><li>some compilers / standard libraries produce an <code>ud2</code> instruction in order to abort the program execution, e.g. <code>core::intrinsics::abort()</code> in Rust.<li>the program tried to execute an instruction that is not available on the CPU it was running on.</ul><p>I've encountered the former one much more often than the latter, but many popular Python libraries code call into optimized C. To be honest, I was pretty sure this was caused by my mess of old, distro-supplied packages (from CentOS 7), in combination with some installed using <code>pip</code>. Of course, it's better to check than to guess:<pre class=language-bash data-lang=bash style=background-color:#2b303b;color:#c0c5ce;><code class=language-bash data-lang=bash><span style=color:#bf616a;>$</span><span> gdb</span><span style=color:#bf616a;> --args</span><span> python3</span><span style=color:#bf616a;> -c </span><span>'</span><span style=color:#a3be8c;>import matplotlib.pyplot as plt; plt.axes()</span><span>'
</span><span style=color:#bf616a;>[snip]
</span><span>(</span><span style=color:#bf616a;>gdb</span><span>) r
</span><span style=color:#bf616a;>Starting</span><span> program: /usr/bin/python3</span><span style=color:#bf616a;> -c</span><span> import</span><span style=color:#96b5b4;>\ </span><span>matplotlib.pyplot</span><span style=color:#96b5b4;>\ </span><span>as</span><span style=color:#96b5b4;>\ </span><span>plt</span><span style=color:#96b5b4;>\;\ </span><span>plt.axes</span><span style=color:#96b5b4;>\(\)
</span><span style=color:#bf616a;>[Thread</span><span> debugging using libthread_db enabled]
</span><span style=color:#bf616a;>Using</span><span> host libthread_db library "</span><span style=color:#a3be8c;>/usr/lib/libthread_db.so.1</span><span>".
</span><span style=color:#bf616a;>[snip]
</span><span>
</span><span style=color:#bf616a;>Thread</span><span> 1 "</span><span style=color:#a3be8c;>python3</span><span>" received signal SIGILL, Illegal instruction.
</span><span style=color:#bf616a;>0x00007ffff556545c</span><span> in dgemm_kernel_PILEDRIVER () </span><span style=color:#bf616a;>from</span><span> /usr/local/lib64/python3.6/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-8a0c371f.3.13.so
</span></code></pre><p><em>Heads-up: for reasons I'll show below, I'm reproducing the issue on my own computer. I'm trying to doctor the outputs, but there might be some inconsistencies.</em><p>It appears to be crashing in a <code>dgemm</code> kernel in OpenBLAS, docs courtesy of LAPACK:<pre style=background-color:#2b303b;color:#c0c5ce;><code><span>DGEMM  performs one of the matrix-matrix operations
</span><span>
</span><span>C := alpha*op( A )*op( B ) + beta*C,
</span><span>
</span><span>where  op( X ) is one of
</span><span>
</span><span>op( X ) = X   or   op( X ) = X**T,
</span><span>
</span><span>alpha and beta are scalars, and A, B and C are matrices, with op( A )
</span><span>an m by k matrix,  op( B )  a  k by n matrix and  C an m by n matrix.
</span></code></pre><p>That's basically a fancy matrix multiplication. More importantly, notice the Piledriver reference in the function name. Piledriver is an AMD microarchitecture from around 2012–2014, which is hopefully not what I'm running on.<p>On the other hand, function names aren't always accurate, especially in highly-optimized code. We can ask GDB to disassemble the current function in order to check the place where the code crashed and it will output a 4890-line monstruosity:<pre class=language-asm data-lang=asm style=background-color:#2b303b;color:#c0c5ce;><code class=language-asm data-lang=asm><span style=color:#8fa1b3;>(gdb) disassemble
</span><span style=color:#8fa1b3;>Dump of assembler code for function dgemm_kernel_PILEDRIVER:
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff555fe00 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>0</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>sub    </span><span style=color:#96b5b4;>$</span><span style=color:#d08770;>0x60</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>rsp
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff555fe04 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>4</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>mov    </span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>rbx</span><span>,</span><span style=color:#8fa1b3;>(%</span><span style=color:#bf616a;>rsp</span><span style=color:#8fa1b3;>)
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff555fe08 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>8</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>mov    </span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>rbp</span><span>,</span><span style=color:#d08770;>0x8</span><span style=color:#8fa1b3;>(%</span><span style=color:#bf616a;>rsp</span><span style=color:#8fa1b3;>)
</span><span style=color:#8fa1b3;>   </span><span>[</span><span style=color:#8fa1b3;>snip</span><span>]
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff5565450 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>22096</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>vmovddup </span><span>-</span><span style=color:#d08770;>0x20</span><span style=color:#8fa1b3;>(%</span><span style=color:#bf616a;>rsi</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>rbp</span><span>,</span><span style=color:#d08770;>8</span><span style=color:#8fa1b3;>)</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm1
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff5565456 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>22102</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>vmovups </span><span>-</span><span style=color:#d08770;>0x80</span><span style=color:#8fa1b3;>(%</span><span style=color:#bf616a;>rdi</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>rax</span><span>,</span><span style=color:#d08770;>8</span><span style=color:#8fa1b3;>)</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm0
</span><span style=color:#8fa1b3;>=> </span><span style=color:#d08770;>0x00007ffff556545c </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>22108</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>vfmaddpd </span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm4</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm1</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm0</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm4
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff5565462 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>22114</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>vmovddup </span><span>-</span><span style=color:#d08770;>0x18</span><span style=color:#8fa1b3;>(%</span><span style=color:#bf616a;>rsi</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>rbp</span><span>,</span><span style=color:#d08770;>8</span><span style=color:#8fa1b3;>)</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm2
</span><span style=color:#8fa1b3;>   </span><span style=color:#d08770;>0x00007ffff5565468 </span><span style=color:#8fa1b3;><</span><span>+</span><span style=color:#d08770;>22120</span><span style=color:#8fa1b3;>>:    </span><span style=color:#b48ead;>vfmaddpd </span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm5</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm2</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm0</span><span>,</span><span style=color:#8fa1b3;>%</span><span style=color:#bf616a;>xmm5
</span></code></pre><p>The crashing opcode was <code>vfmaddpd</code>. I've never encountered it before — not exactly surprising — but notice how it has four operands, which is pretty rare in <code>x86</code> instructions. That said, it's pretty easy to guess what it does if you can unpack its mnemonic:<ul><li><code>v</code>: vector (SIMD) instruction, meaning it operates not on scalars, but on vector registers with multiple values<li><code>fma</code>: fused multiply-add, that is, a multiplication and addition in one step, for better performance, precision, or both<li><code>p</code>: packed, meaning it uses all the values in the registers (as opposed to the scalar ones, which only touch a single value in a vector register)<li><code>d</code>: double-precision, i.e. 64-bit floating-point numbers</ul><p>Peeking at <a href=https://www.amd.com/system/files/TechDocs/43479.pdf>the docs</a>, the instruction name is <code>Multiply and Add Packed Double-Precision Floating-Point</code>:<pre class=language-asm data-lang=asm style=background-color:#2b303b;color:#c0c5ce;><code class=language-asm data-lang=asm><span style=color:#b48ead;>VFMADDPD </span><span style=color:#8fa1b3;>dest</span><span>, </span><span style=color:#8fa1b3;>src1</span><span>, </span><span style=color:#8fa1b3;>src2</span><span>, </span><span style=color:#8fa1b3;>src3 # dest = (src1 </span><span>* </span><span style=color:#8fa1b3;>src2) </span><span>+ </span><span style=color:#8fa1b3;>src3
</span></code></pre><p>It doesn't matter for us, but <code>xmm</code> are 128-bit registers, so it's working on pairs <code>double</code>s.<p>This is an <code>FMA4</code> (four-operand FMA) instruction, which has a bit of a weird history. It was introduced by AMD, but Intel never implemented it. Instead, Intel added their own <code>FMA3</code> (three-operand) instructions, which look like <code>vfmadd213pd xmm0, xmm1, xmm2</code> and don't have a separate destination. If you're wondering, <code>213</code> specifies the operand order, the one here doing <code>xmm0 = xmm1 * xmm0 + xmm2</code>.<p>In any case, AMD dropped FMA4 in 2017 with the Zen microarchitecture, which probably caused some confusion because the instructions still work, but give the wrong results sometimes.<p>By this point, we have a theory: our CPU does not support FMA4, but OpenBLAS (used by <code>numpy</code>, used by <code>matplotlib</code>) thinks it does, picks up that code path and happily crashes.<h2 id=opteron>Opteron</h2><p>I don't have the <code>/proc/cpuinfo</code> output any more (see below), but the model name was <code>AMD Opteron 63xx class</code>, or something similar. This is a virtual machine, so I'm assuming the hypervisor was reporting an older CPU model.<p>Software can detect CPU features in a fine-grained way (using the <code>cpuid</code> instruction), but OpenBLAS only checks for the CPU model <a href=https://github.com/xianyi/OpenBLAS/blob/d33fc32cf30cf1262030c93fa44c72ca8ab27681/cpuid_x86.c#L1288-L1327>here</a> and then again <a href=https://github.com/xianyi/OpenBLAS/blob/d33fc32cf30cf1262030c93fa44c72ca8ab27681/driver/others/dynamic.c#L353-L385>here</a>. There actually was an <a href=https://en.wikipedia.org/wiki/List_of_AMD_Opteron_processors#3300-,_4300-_&_6300-series_Opterons>Opteron 6300</a> series, which matches the reported model name and did support <code>FMA4</code>.<p>I also tried <a href=https://gist.github.com/rindeal/81198b1cf8f55c356743#file-cpuid-dump2-c>some code</a> that checks specifically for <code>FMA4</code>. I can't run it again, but indeed, it reported no <code>FMA4</code> support.<h2 id=the-fix-and-a-twist>The fix and a twist</h2><p>Knowing the problem, the fix was pretty simple. Fortunately, OpenBLAS supports overriding the CPU-specific code paths through an environment variable, and setting <code>OPENBLAS_CORETYPE=ZEN</code> made it work. I still filed <a href=https://github.com/xianyi/OpenBLAS/issues/3638>an issue</a> against the OpenBLAS repo, in case anyone else runs into this.<p>Unfortunately, one day later, I can't run my notebook because the <code>/</code> partition is mounted read-only, the file system might be corrupted, and pretty much everything is broken. But for once, it's not my fault 😅.<pre style=background-color:#2b303b;color:#c0c5ce;><code><span>FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/user']
</span></code></pre><p>Yes, I could mount <code>tmpfs</code> into <code>/tmp</code>, but it doesn't matter.<p>And the Opteron magically turned into an EPYC:<pre style=background-color:#2b303b;color:#c0c5ce;><code><span>processor       : 0
</span><span>vendor_id       : AuthenticAMD
</span><span>cpu family      : 23
</span><span>model           : 49
</span><span>model name      : AMD EPYC 7502P 32-Core Processor
</span><span>stepping        : 0
</span><span>microcode       : 0x1000065
</span><span>cpu MHz         : 2500.001
</span><span>cache size      : 512 KB
</span><span>physical id     : 0
</span><span>siblings        : 1
</span><span>core id         : 0
</span><span>cpu cores       : 1
</span><span>apicid          : 0
</span><span>initial apicid  : 0
</span><span>fpu             : yes
</span><span>fpu_exception   : yes
</span><span>cpuid level     : 16
</span><span>wp              : yes
</span><span>flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm art rep_good nopl extd_apicid eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core retpoline_amd ssbd ibrs ibpb vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 virt_ssbd arat npt nrip_save umip spec_ctrl
</span><span>bogomips        : 5000.00
</span><span>TLB size        : 1024 4K pages
</span><span>clflush size    : 64
</span><span>cache_alignment : 64
</span><span>address sizes   : 40 bits physical, 48 bits virtual
</span><span>power management:
</span></code></pre><h2 id=conclusion>Conclusion</h2><p>Just a couple of things:<ul><li>feature detection is better than version checking not only when writing JavaScript, but even when targeting low-level code<li>it's a good idea to provide escape hatches from optimized code paths<li>the CPU is a lie</ul><hr><p>And by the way, if you enjoyed this short post, please consider <a href=https://www.buymeacoffee.com/lnicolaq>buying me a coffee</a>.</div></div>